{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import  os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import beta, ks_2samp\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from model import load_object\n",
    "from sentiments import parse_adj_supersense, parse_verb_supersense, parse_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = './models/lexical_pos_all-data/'\n",
    "dir_jj = f'{MODEL_DIR}/jj'\n",
    "dir_dobj = f'{MODEL_DIR}/dobj'\n",
    "dir_nsubj = f'{MODEL_DIR}/nsubj'\n",
    "\n",
    "DESCENDING_ETA = True\n",
    "TERMS_PER_TOPIC = 200\n",
    "\n",
    "L1_REG = 0\n",
    "KL_REG = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def read_results(dir):\n",
    "  grid = os.listdir(dir)\n",
    "  results = {}\n",
    "  for fname in grid:\n",
    "    if 'config_dict.pkl' not in fname and fname.endswith('pkl'):\n",
    "      r = load_object(os.path.join(dir, fname))\n",
    "      r['w_vocab'] = r.pop('adj_vocab', r.pop('w_vocab', None))\n",
    "      results[(r['l1_reg'], r['kl_reg'])] = r\n",
    "  results = OrderedDict(sorted(results.items(), key=lambda kv: kv[0]))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def top_n_terms(x, inv_vocab, n=25, vals=False, desc=False):\n",
    "  out = x.round(1) if vals else inv_vocab\n",
    "  idx = x.argsort()\n",
    "  if desc:\n",
    "    idx = idx[::-1]  \n",
    "  return [out[i] for i in idx[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def perm_test_two_sided(x, y, nmc=30000, p_ci=0.99, abs=False):\n",
    "  '''\n",
    "  Ported from `perm` package in R\n",
    "  '''\n",
    "  w = np.concatenate([x, y])\n",
    "  z = np.concatenate([np.ones_like(x), np.zeros_like(y)])\n",
    "  \n",
    "  t0 = (w * z).sum()\n",
    "  ti = np.zeros(nmc)\n",
    "  for j in range(nmc):\n",
    "    ti[j] = (w * np.random.permutation(z)).sum()\n",
    "    \n",
    "  mu = ti.mean()\n",
    "  ti = ti - mu\n",
    "  t0 = t0 - mu\n",
    "  \n",
    "  s_lte = (ti <= t0).sum()\n",
    "  s_gte = (ti >= t0).sum()\n",
    "  s_abs = (np.abs(ti) >= np.abs(t0)).sum()\n",
    "  \n",
    "  s = s_abs if abs else min(s_lte, s_gte) \n",
    "  r = 1 if abs else 2\n",
    "  \n",
    "  # calculate CI\n",
    "  alpha = 1 - p_ci\n",
    "  ci = r * np.array([\n",
    "    beta.ppf(alpha / 2, s, nmc - s + 1),\n",
    "    beta.ppf(1 - alpha / 2, s + 1, nmc - s)\n",
    "  ])\n",
    "  \n",
    "  p_lte = (s_lte + 1) / (nmc + 1)\n",
    "  p_gte = (s_gte + 1) / (nmc + 1)\n",
    "  p_abs = (s_abs + 1) / (nmc + 1)\n",
    "  p = p_abs if abs else min(1, 2 * min(p_lte, p_gte))\n",
    "  \n",
    "  return x.mean() - y.mean(), p, ci\n",
    "\n",
    "\n",
    "def softmax(x, axis=0):\n",
    "  e_x = np.exp(x - np.max(x))\n",
    "  return e_x / e_x.sum(axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "results_jj = read_results(dir_jj)\n",
    "results_dobj = read_results(dir_dobj)\n",
    "results_nsubj = read_results(dir_nsubj)\n",
    "\n",
    "senses_adj = parse_adj_supersense('data/sentiments/word_types.predicted')\n",
    "senses_verb = parse_verb_supersense('data/sentiments/semcor_noun_verb.supersenses.en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "sent_data = pd.read_csv(\n",
    "    './models/vae_full_primedprior_softmax/sent_dict.csv',\n",
    ").drop('from_vae', axis=1)\n",
    "sent_data.columns = ['word', 'pos', 'neu', 'neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results_list, k, desc, exclude_kl=[0], exclude_l1=[0.1, 1.0, 10.0, 100.0]):\n",
    "  '''\n",
    "  Process results across models and store in dataframe\n",
    "  '''\n",
    "  topic_set = []\n",
    "  pos, neg, neu = 0, 2, 1\n",
    "\n",
    "  for (l1, kl), result in results_list.items():\n",
    "    if l1 in exclude_l1:\n",
    "      continue\n",
    "    if kl in exclude_kl:\n",
    "      continue\n",
    "\n",
    "    model_result = []\n",
    "    for fem in (0, 1):\n",
    "      \n",
    "      # Hack to accomodate sent-free\n",
    "      if len(result['eta_fem_sent'].squeeze().shape) == 2:\n",
    "        result['eta_fem_sent'] = np.tile(result['eta_fem_sent'].squeeze(), [3, 1, 1])\n",
    "      \n",
    "      eta = result['eta_fem_sent'].squeeze()[:, fem, :]\n",
    "      inv_vocab = dict(zip(result['w_vocab'].values(), result['w_vocab'].keys()))\n",
    "      gender = 'fem' if fem else 'masc'\n",
    "      model_result.append(pd.DataFrame({\n",
    "        f'{gender}_pos_terms': top_n_terms(eta[pos, :], inv_vocab, k, desc=desc),\n",
    "        f'{gender}_pos_values': top_n_terms(eta[pos, :], inv_vocab, k, vals=True, desc=desc),\n",
    "        f'{gender}_neg_terms': top_n_terms(eta[neg, :], inv_vocab, k, desc=desc),\n",
    "        f'{gender}_neg_values': top_n_terms(eta[neg, :], inv_vocab, k, vals=True, desc=desc),\n",
    "        f'{gender}_neu_terms': top_n_terms(eta[neu, :], inv_vocab, k, desc=desc),\n",
    "        f'{gender}_neu_values': top_n_terms(eta[neu, :], inv_vocab, k, vals=True, desc=desc),\n",
    "      }))\n",
    "    model_result = pd.concat(model_result, axis=1)\n",
    "    model_result['l1'] = l1\n",
    "    model_result['kl'] = kl\n",
    "    topic_set.append(model_result)\n",
    "  \n",
    "  return pd.concat(topic_set, ignore_index=True, axis=0)\n",
    "\n",
    "def combine_results(results_list, k, desc, exclude_kl=[0.], exclude_l1=[]):\n",
    "  '''\n",
    "  Combine results across all models\n",
    "  '''\n",
    "  topic_set = process_results(results_list, k, desc=desc, exclude_kl=exclude_kl, exclude_l1=exclude_l1)\n",
    "  grouped_topics = []\n",
    "  \n",
    "  for gender in ('masc', 'fem'):\n",
    "    for sent in ('pos', 'neg', 'neu'):\n",
    "      col_term, col_val, col_count = (\n",
    "        f'{gender}_{sent}_terms', f'{gender}_{sent}_values', f'{gender}_{sent}_counts'\n",
    "      )\n",
    "      \n",
    "      grouped = (\n",
    "        topic_set[[col_term, col_val]]\n",
    "            .loc[topic_set[col_val] > 0] # TODO: Think about excluding zeros; \n",
    "            .groupby(col_term, as_index=False)\n",
    "           .agg(['count', 'mean'])\n",
    "      )\n",
    "      grouped.columns = grouped.columns.droplevel(0)\n",
    "      grouped.columns = [col_count, col_val]\n",
    "      grouped = (\n",
    "        grouped.sort_values([col_count, col_val], ascending=False)\n",
    "               .reset_index()\n",
    "      )\n",
    "      grouped_topics.append(grouped[[col_term, col_val]])\n",
    "      grouped_topics\n",
    "  grouped_topics = pd.concat(grouped_topics, axis=1)\n",
    "  return grouped_topics.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_jj = combine_results(results_jj, k=TERMS_PER_TOPIC, desc=DESCENDING_ETA)#, exclude_kl=[])\n",
    "topics_dobj = combine_results(results_dobj, k=TERMS_PER_TOPIC, desc=DESCENDING_ETA)#, exclude_kl=[])\n",
    "topics_nsubj = combine_results(results_nsubj, k=TERMS_PER_TOPIC, desc=DESCENDING_ETA)#, exclude_kl=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics_jj.head(25).round(1).to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics_dobj.head(25).round(1).to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics_nsubj.head(25).round(1).to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Evaluate sense differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def map_topics_to_senses(topics, sense_data):\n",
    "  '''\n",
    "  Map the top n eta terms to the sense data for a given result\n",
    "  '''      \n",
    "  # TODO: account for sent-free data\n",
    "  term_sense_data = []  \n",
    "  for gender in ['masc', 'fem']:\n",
    "    for sent in ['pos', 'neg', 'neu']:      \n",
    "      term_senses = map_terms_to_senses(topics[f'{gender}_{sent}_terms'].values, sense_data)\n",
    "      term_senses['gender'] = gender\n",
    "      term_senses['sent'] = sent\n",
    "      term_sense_data.append(term_senses)\n",
    "      \n",
    "  return pd.concat(term_sense_data, ignore_index=True)\n",
    "\n",
    "def map_terms_to_senses(terms, sense_data):\n",
    "  '''\n",
    "  Map a set of terms to sense data\n",
    "  ''' \n",
    "  term_senses = (\n",
    "    pd.DataFrame(terms)\n",
    "      .merge(sense_data, how='inner', left_on=0, right_on='word')\n",
    "      .reset_index()\n",
    "  )\n",
    "  return term_senses\n",
    "\n",
    "def topic_sense_perm_test(term_sense_data, sense_types, nmc, abs, p_ci, alpha=0.05):\n",
    "  '''\n",
    "  Run the permutation test on the mapped data\n",
    "  '''\n",
    "  perm_test_data = []\n",
    "  for sent in term_sense_data.sent.unique():\n",
    "    for sense in sense_types:\n",
    "      masc_values = term_sense_data.loc[\n",
    "        (term_sense_data.gender == 'masc') & (term_sense_data.sent == sent)\n",
    "      ][sense].values\n",
    "      \n",
    "      fem_values = term_sense_data.loc[\n",
    "        (term_sense_data.gender == 'fem') & (term_sense_data.sent == sent)\n",
    "      ][sense].values\n",
    "      \n",
    "      diff, p, ci = perm_test_two_sided(masc_values, fem_values, nmc=nmc, p_ci=p_ci, abs=abs)\n",
    "      sig = '*' if p <= (alpha / len(sense_types)) else '' # bonferonni\n",
    "      # p, sig, ci = 0, '', 0 # AVOID P-HACKING\n",
    "      perm_test_data.append([sent, sense, masc_values.mean(), fem_values.mean(), f'{p:0.3f}{sig}', np.round(ci, 3)])\n",
    "  \n",
    "  return pd.DataFrame(perm_test_data, columns=['Sentiment', 'Sense', 'Mean Masc', 'Mean Fem', 'p', 'C.I.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Perm Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_sense_data_jj = map_topics_to_senses(\n",
    "  topics=topics_jj,\n",
    "  sense_data=senses_adj,\n",
    ")\n",
    "term_sense_data_dobj = map_topics_to_senses(\n",
    "  topics=topics_dobj,\n",
    "  sense_data=senses_verb,\n",
    ")\n",
    "term_sense_data_nsubj = map_topics_to_senses(\n",
    "  topics=topics_nsubj,\n",
    "  sense_data=senses_verb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "sense_types_adj = [c for c in senses_adj.columns if c not in ['word', 'class']]\n",
    "sense_types_verb = [c for c in senses_verb.columns if c not in ['word']]\n",
    "\n",
    "perm_test_data_jj = topic_sense_perm_test(\n",
    "  term_sense_data_jj,\n",
    "  sense_types=sense_types_adj,\n",
    "  nmc=30000,\n",
    "  abs=False,\n",
    "  p_ci=0.99\n",
    ")\n",
    "\n",
    "perm_test_data_dobj = topic_sense_perm_test(\n",
    "  term_sense_data_dobj,\n",
    "  sense_types=sense_types_verb,\n",
    "  nmc=30000,\n",
    "  abs=False,\n",
    "  p_ci=0.99\n",
    ")\n",
    "\n",
    "perm_test_data_nsubj = topic_sense_perm_test(\n",
    "  term_sense_data_nsubj,\n",
    "  sense_types=sense_types_verb,\n",
    "  nmc=30000,\n",
    "  abs=False,\n",
    "  p_ci=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "perm_test_data_jj.loc[perm_test_data_jj.p.str.contains(r'\\*')]\n",
    "#print(perm_test_data_jj.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "sig_sense_jj = perm_test_data_jj.loc[perm_test_data_jj.p.str.contains(r'\\*')].copy()\n",
    "\n",
    "\n",
    "sig_sense_jj['label'] = sig_sense_jj.Sentiment.str.upper() + '--' + sig_sense_jj.Sense.str.replace('MISCELLANEOUS', 'MISC')\n",
    "sig_sense_jj = sig_sense_jj.rename({'Mean Masc': 'Masc', 'Mean Fem': 'Fem'}, axis=1)\n",
    "\n",
    "# tex fonts\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "ax = sig_sense_jj.plot(x=\"label\", y=[\"Masc\", \"Fem\"], kind=\"bar\", colormap=\"Set2\")\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=15)\n",
    "plt.ylim((0, 0.25))\n",
    "\n",
    "plt.savefig(f\"./jj-sense.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "perm_test_data_dobj.loc[perm_test_data_dobj.p.str.contains(r'\\*')]\n",
    "# print(perm_test_data_dobj.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "perm_test_data_nsubj.loc[perm_test_data_nsubj.p.str.contains(r'\\*')]\n",
    "#print(perm_test_data_nsubj.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sig_sense_nsubj = perm_test_data_nsubj.loc[perm_test_data_nsubj.p.str.contains(r'\\*')].copy()\n",
    "\n",
    "replacer = re.compile('verb_')\n",
    "sig_sense_nsubj['label'] = (\n",
    "  sig_sense_nsubj.Sentiment.str.upper() \n",
    "  + '--' \n",
    "  + sig_sense_nsubj.Sense.str.replace(replacer, '').replace('communication', 'comm.').str.upper()\n",
    ")\n",
    "sig_sense_nsubj = sig_sense_nsubj.rename({'Mean Masc': 'Masc', 'Mean Fem': 'Fem'}, axis=1)\n",
    "\n",
    "# tex fonts\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "ax = sig_sense_nsubj.plot(x=\"label\", y=[\"Masc\", \"Fem\"], kind=\"bar\", colormap=\"Set2\")\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=15)\n",
    "plt.ylim((0, 0.25))\n",
    "\n",
    "plt.savefig(f\"./nsubj-sense.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "for a, v in zip_longest(sense_types_adj, sense_types_verb):\n",
    "  v = v.replace(\"verb_\", \"\").title()\n",
    "  a = a.title() if a else ''\n",
    "  print(f'{a} & {v} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Human Annotation Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_fem_sent_mean_jj = np.mean([r['eta_fem_sent'] for r in results_jj.values()], axis=0)\n",
    "eta_lemma_mean_jj = np.mean([r['eta_lemma'] for r in results_jj.values()], axis=0)\n",
    "eta_plural_mean_jj = np.mean([r['eta_plural'] for r in results_jj.values()], axis=0)\n",
    "sigma_mean_jj = np.mean([r['sigma'] for r in results_jj.values()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import process_data, load_object, ModelConfig\n",
    "\n",
    "config_dict = load_object(f'{MODEL_DIR}/config_dict.pkl')\n",
    "config = ModelConfig(**config_dict)\n",
    "config.sent_fpath = './models/vae_full_primedprior_softmax/sent_dict.csv'\n",
    "data, _, _ = process_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "eta_fem_sent_tf = tf.constant(eta_fem_sent_mean_jj)\n",
    "eta_lemma_tf = tf.constant(eta_lemma_mean_jj)\n",
    "eta_plural_tf = tf.constant(eta_plural_mean_jj)\n",
    "sigma_tf = tf.constant(sigma_mean_jj)\n",
    "\n",
    "m_tf =  tf.constant(results_jj[(0.0, 0.00001)]['m'])\n",
    "\n",
    "p_w_given_sent_noun_tf = tf.nn.softmax(m_tf + eta_fem_sent_tf + eta_lemma_tf + eta_plural_tf, axis=-1)\n",
    "prob_sent_given_gend_tf = tf.nn.softmax(sigma_tf, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  p_w_given_sent_noun, prob_sent_given_gend = sess.run([p_w_given_sent_noun_tf, prob_sent_given_gend_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginalize out the lexical features\n",
    "p_w_lex_given_sent = (\n",
    "  p_w_given_sent_noun \n",
    "  * data['lemma_freq'].reshape(1, 1, -1, 1, 1)\n",
    "  * data['plural_freq'].reshape(1, 1, 1, 2, 1)\n",
    ")\n",
    "p_w_given_sent_gend = p_w_lex_given_sent.sum(axis=(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "human_eval = pd.read_csv('./data/human-eval/acl.txt', sep=' ', names=['gend', 'word', 'sent'])\n",
    "\n",
    "vocab = results_jj[(L1_REG, KL_REG)]['w_vocab']\n",
    "sent_map = {0: 'pos', 2: 'neg', 1: 'neu'}\n",
    "gend_map = {0: 'masc', 1: 'fem'}\n",
    "\n",
    "for i, row in human_eval.iterrows():\n",
    "  try:\n",
    "    word_idx = vocab[row.word]\n",
    "  except KeyError:\n",
    "    continue\n",
    "  p_w = p_w_given_sent_gend[:, :, word_idx]\n",
    "  for gend_idx, gend in gend_map.items():\n",
    "    p_data = p_w[:, gend_idx] * prob_sent_given_gend[:, gend_idx]\n",
    "    norm = (p_w_given_sent_gend[:, gend_idx, :] * prob_sent_given_gend[:, gend_idx].reshape(3, 1)).max()\n",
    "    human_eval.at[i, f'eta_{gend}'] = p_data.sum()\n",
    "    for sent_idx, sent in sent_map.items():\n",
    "      pass\n",
    "      #p_data = p_w[sent_idx, gend_idx] * prob_sent_given_gend[sent_idx, gend_idx]\n",
    "      #human_eval.at[i, f'eta_{gend}_{sent}'] = p_data\n",
    "      \n",
    "human_eval = human_eval.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "human_gend_evals = (human_eval.gend == 'fem') * 1\n",
    "model_gend_evals = (human_eval.eta_fem > human_eval.eta_masc) * 1\n",
    "diff = (human_eval.eta_fem - human_eval.eta_masc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "spearmanr(human_gend_evals, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "williamsbest = pd.read_csv('./data/human-eval/adjectives_williamsbest.csv')\n",
    "williamsbest = williamsbest.groupby('word', as_index=False).mean()\n",
    "\n",
    "vocab = results_jj[(L1_REG, KL_REG)]['w_vocab']\n",
    "sent_map = {0: 'pos', 2: 'neg', 1: 'neu'}\n",
    "gend_map = {0: 'masc', 1: 'fem'}\n",
    "\n",
    "for i, row in williamsbest.iterrows():\n",
    "  try:\n",
    "    word_idx = vocab[row.word.lower()]\n",
    "  except KeyError:\n",
    "    continue\n",
    "  p_w = p_w_given_sent_gend[:, :, word_idx]\n",
    "  for gend_idx, gend in gend_map.items():\n",
    "    p_data = p_w[:, gend_idx] * prob_sent_given_gend[:, gend_idx]\n",
    "    norm = (p_w_given_sent_gend[:, gend_idx, :] * prob_sent_given_gend[:, gend_idx].reshape(3, 1)).max()\n",
    "    williamsbest.at[i, f'eta_{gend}'] = p_data.sum()\n",
    "    for sent_idx, sent in sent_map.items():\n",
    "      pass\n",
    "      #p_data = p_w[sent_idx, gend_idx] * prob_sent_given_gend[sent_idx, gend_idx]\n",
    "      #human_eval.at[i, f'eta_{gend}_{sent}'] = p_data\n",
    "      \n",
    "williamsbest = williamsbest.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "wb_norm = (\n",
    "  (williamsbest.transformed_score + np.abs(williamsbest.transformed_score.min())) \n",
    "  / np.max(williamsbest.transformed_score + np.abs(williamsbest.transformed_score.min()))\n",
    ")\n",
    "model_diff = williamsbest.eta_fem - williamsbest.eta_masc\n",
    "model_diff_shift = model_diff + np.abs(model_diff.min())\n",
    "model_norm = model_diff_shift / np.max(model_diff_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "spearmanr(williamsbest.transformed_score, model_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "np.mean((williamsbest.transformed_score < 0) == (williamsbest.eta_masc > williamsbest.eta_fem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Bolukbasi correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../debiaswe-master/debiaswe/')\n",
    "import debias\n",
    "import we\n",
    "import json\n",
    "\n",
    "embeddings_fpath = 'C:/users/ahoyl/datasets/word-embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "embeddings = we.WordEmbedding(embeddings_fpath)\n",
    "\n",
    "with open('../debiaswe-master/data/definitional_pairs.json', 'r') as f:\n",
    "  definitional = json.load(f)\n",
    "  \n",
    "gender_direction = we.doPCA(definitional, embeddings).components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "max_freq = 100000\n",
    "topn = 5000\n",
    "dots = np.square(embeddings.vecs[:max_freq].dot(gender_direction))\n",
    "thresh = sorted(dots)[-topn]\n",
    "words = [\n",
    "  w for w, dot in zip(embeddings.words, dots)\n",
    "  if dot >= thresh and len(wn.synsets(w, pos='a')) > 0\n",
    "]\n",
    "sorted(words, key=lambda w: embeddings.v(w).dot(gender_direction))[-topn:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "williamsbest = pd.read_csv('./data/human-eval/adjectives_williamsbest.csv')\n",
    "williamsbest = williamsbest.groupby('word', as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, word in williamsbest[['word']].itertuples():\n",
    "  try:\n",
    "    v = embeddings.v(word)\n",
    "  except KeyError:\n",
    "    v = np.nan\n",
    "    continue\n",
    "  williamsbest.at[idx, 'sim'] = gender_direction.dot(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(williamsbest.dropna().sim, williamsbest.dropna().transformed_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Evaluate sentiment differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def map_topics_to_sents(topics, sent_data, trained_with_sent):\n",
    "  '''\n",
    "  Map the sentiment data\n",
    "  '''\n",
    "    \n",
    "  term_sent_data = []\n",
    "  for gender in ('masc', 'fem'):\n",
    "      terms = topics[f'{gender}_neu_terms'].values\n",
    "      if trained_with_sent:\n",
    "        # should likely update to be sorted better\n",
    "        cols = [c for c in topics.columns if gender in c]\n",
    "        terms = topics[[c for c in cols if 'terms' in c]].values.flatten()\n",
    "        vals_idx = topics[[c for c in cols if 'values' in c]].values.flatten().argsort()[-TERMS_PER_TOPIC:]\n",
    "        terms = terms[vals_idx]\n",
    "        print(terms[-5:])\n",
    "        \n",
    "      term_sents = (\n",
    "        pd.DataFrame(terms)\n",
    "          .merge(sent_data, how='inner', left_on=0, right_on='word')\n",
    "          .reset_index()\n",
    "      )\n",
    "      term_sents['gender'] = gender\n",
    "      term_sent_data.append(term_sents)\n",
    "      \n",
    "  return pd.concat(term_sent_data, ignore_index=True)\n",
    "\n",
    "def topic_sent_perm_test(term_sense_data, nmc, abs, p_ci, alpha=0.05):\n",
    "  '''\n",
    "  Run the permutation test on the mapped data\n",
    "  '''\n",
    "  perm_test_data = []\n",
    "  for sent in ('pos', 'neg', 'neu'):\n",
    "      masc_values = term_sense_data.loc[(term_sense_data.gender == 'masc')][sent].values\n",
    "      fem_values = term_sense_data.loc[(term_sense_data.gender == 'fem')][sent].values\n",
    "      diff, p, ci = perm_test_two_sided(masc_values, fem_values, nmc=nmc, p_ci=p_ci, abs=abs)\n",
    "      sig = '*' if p <= (alpha / 3.) else '' # bonferonni\n",
    "      # p, sig, ci = 0, '', 0 # AVOID P-HACKING\n",
    "      \n",
    "      perm_test_data.append([sent, masc_values.mean(), fem_values.mean(), f'{p:0.3f}{sig}', np.round(ci, 3)])\n",
    "  \n",
    "  return pd.DataFrame(perm_test_data, columns=['Sentiment', 'Mean Masc', 'Mean Fem', 'p', 'C.I.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# normalize the sentiment data\n",
    "sent_data[['pos', 'neg', 'neu']] = sent_data[['pos', 'neg', 'neu']].apply(lambda x: x / sent_data.sum(1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_with_sent = not np.all(topics_jj['masc_neg_terms'] == topics_jj['masc_pos_terms'])\n",
    "trained_with_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "term_sent_data_jj = map_topics_to_sents(\n",
    "  topics=topics_jj,\n",
    "  sent_data=sent_data,\n",
    "  trained_with_sent=trained_with_sent,\n",
    ")\n",
    "term_sent_data_dobj = map_topics_to_sents(\n",
    "  topics=topics_dobj,\n",
    "  sent_data=sent_data,\n",
    "  trained_with_sent=trained_with_sent,\n",
    ")\n",
    "term_sent_data_nsubj = map_topics_to_sents(\n",
    "  topics=topics_nsubj,\n",
    "  sent_data=sent_data,\n",
    "  trained_with_sent=trained_with_sent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "perm_test_data_jj = topic_sent_perm_test(\n",
    "  term_sent_data_jj,\n",
    "  nmc=30000,\n",
    "  abs=False,\n",
    "  p_ci=0.99\n",
    ")\n",
    "\n",
    "perm_test_data_dobj = topic_sent_perm_test(\n",
    "  term_sent_data_dobj,\n",
    "  nmc=30000,\n",
    "  abs=False,\n",
    "  p_ci=0.99\n",
    ")\n",
    "\n",
    "perm_test_data_nsubj = topic_sent_perm_test(\n",
    "  term_sent_data_nsubj,\n",
    "  nmc=30000,\n",
    "  abs=False,\n",
    "  p_ci=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "perm_test_data_jj.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "perm_test_data_dobj.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "perm_test_data_nsubj.round(2)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
